# Perfecting adversarial examples for common image data sets

This is a Deep Learning mini project that leverages well known image datasets such as MNIST, CIFAR-100 etc  to solve the below problem:
Defense against adversarial examples

  Problem: Several image classification systems have been shown to be tricked into gross misclassification by making small, sometimes imperceptible changes to the input image, e.g., tricking the system into thinking a panda is a gibbon. An open problem is how to modify an architecture or regularizer to make classifiers more robust against such minor changes. We use a pretrained CNN in addition to running a few experiments of our own . The attack defense machinery was generated using the cleverhans library as a backend, a Python library to benchmark machine learning systems' vulnerability to adversarial examples. 
  https://github.com/tensorflow/cleverhans
  The results show highest adversarial training accuracy on MNIST.
  
 # PPT link
 https://drive.google.com/file/d/1KR-NU_s2lYjdDHEkPeIhSenY1PfgQJoX/view?usp=sharing
